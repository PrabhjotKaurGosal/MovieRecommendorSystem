{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendor System using k-Nearest Neighbor method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import matplotlib as plt\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read training data (raw)\n",
    "1. Read from the .txt files\n",
    "2. convert to .csv\n",
    "3. Read the .csv into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame to store all imported data\n",
    "if not os.path.isfile('data.csv'):\n",
    "    data = open('data.csv', mode='w')\n",
    "\n",
    "files = ['/content/drive/My Drive/NetflixChallenge/archive/combined_data_1.txt',\n",
    "         #'/content/drive/My Drive/NetflixChallenge/archive/combined_data_2.txt',\n",
    "     #     '/content/drive/My Drive/NetflixChallenge/archive/combined_data_3.txt',  \n",
    "       #   '/content/drive/My Drive/NetflixChallenge/archive/combined_data_4.txt'\n",
    "        ]\n",
    "\n",
    "# Remove the line with movie_id: and add a new column of movie_id\n",
    "# Combine all data files into a csv file\n",
    "for file in files:\n",
    "    print(\"Opening file: {}\".format(file))\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.endswith(':'):\n",
    "                movie_id = line.replace(':', '')\n",
    "            else:\n",
    "                size = len(line)\n",
    "                line = line[:size - 11]\n",
    "                data.write(movie_id + ',' + line)\n",
    "                data.write('\\n')\n",
    "data.close()\n",
    "\n",
    "# Read all data into a pd dataframe\n",
    "df = pd.read_csv('data.csv', names=['movie_id', 'user_id','rating'])\n",
    "print(\"The data count is as follows: \")\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Training data (raw)\n",
    "Top 'k' rated movies (this data is extracted for visualization purposes only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_k = 25\n",
    "\n",
    "group = df.groupby('movie_id')['rating'].count()\n",
    "Top_k_movies = group.sort_values(ascending=False)[:Top_k]\n",
    "#print(Top_k_movies)\n",
    "Top_k_movies.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top K users (who rated more often)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_K_users = 5000\n",
    "group = df.groupby('user_id')['rating'].count()\n",
    "Top_K_users = group.sort_values(ascending=False)[:Top_K_users]\n",
    "print(Top_K_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only Top_K_users for further processing\n",
    "lite_rating_df = df.join(Top_K_users, rsuffix='_total', how='inner', on='user_id')\n",
    "print(lite_rating_df.nunique())\n",
    "lite_rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dataframe to numpy matrices\n",
    "X = lite_rating_df[['user_id', 'movie_id']].values\n",
    "y = lite_rating_df['rating'].values\n",
    "print(X)\n",
    "print(y)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame to store all imported data\n",
    "if not os.path.isfile('Testdata.csv'):\n",
    "    data = open('Testdata.csv', mode='w')\n",
    "\n",
    "files = ['/content/drive/My Drive/NetflixChallenge/archive/probe.txt']\n",
    "\n",
    "# Remove the line with movie_id: and add a new column of movie_id\n",
    "# Combine all data files into a csv file\n",
    "for file in files:\n",
    "  print(\"Opening file: {}\".format(file))\n",
    "  with open(file) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.endswith(':'):\n",
    "            movie_id = line.replace(':', '')\n",
    "        else:\n",
    "            size = len(line)\n",
    "            line = line[:size]\n",
    "            data.write(movie_id + ',' + line)\n",
    "            data.write('\\n')\n",
    "data.close()\n",
    "\n",
    "# Read all data into a pd dataframe\n",
    "df_Test = pd.read_csv('Testdata.csv', names=['movie_id', 'user_id'])\n",
    "print(\"The data count is as follows: \")\n",
    "print(df_Test.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pre-processing was necessary because of the pre-processing step that was applied to the Training set\n",
    "# When pre-processing was applied to the training set, only Top-K users were kept\n",
    "# So, some of the data was lost from the training set for the users in the Test Set.\n",
    "# This meant that same pre-processing had to be applied to the Test Set to apply User-User collaborative filtering\n",
    "lite_rating_df_Test = df_Test.join(Top_K_users, how='inner', on='user_id')\n",
    "print(lite_rating_df_Test.nunique())\n",
    "lite_rating_df_Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, prediction was only made for subset of the data in the Test Set for the sake of time and simply to evaluate the efficacy of the method proposed in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSet = lite_rating_df_Test[['movie_id', 'user_id']].values\n",
    "TestSet_truncated = TestSet[TestSet[:,0]<=1000] \n",
    "print(TestSet_truncated)\n",
    "print(TestSet_truncated.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the ratings of various movies for the users in the Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: \n",
    "Collaborative Filtering based on User-User Similarity In this method, the user for whom the prediction is to be made is compared with other users in the training set. The method uses Pearson Coefficient to find the similarity of this user with other users. Then, top 'NN' similar users are picked and the final prediction is made based on the similar users. (The 'NN' is defined in the steps below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "MAS_cumulative_UserUserMtd1 = 0 # Cumulative Error (Mean Absolute Error)\n",
    "RMSE_cumulative_UserUserMtd1 = 0 # Cumulatve Error (Root Mean Squared Error)\n",
    "Total_predictions_UserUserMtd1 = [] #Store all predictions made using Method 1\n",
    "Actual_rating_ALL = [] # Store all actual predictions\n",
    "Target_Users_ALL = []\n",
    "Target_Movie_ALL = []\n",
    "\n",
    "# Make predictions for all the data in the TestSet_Truncated\n",
    "#for i in range(0, TestSet_truncated.shape[0]):\n",
    "for i in range(0,100):\n",
    "    target_movie = TestSet_truncated[i,:][0]\n",
    "    target_user = TestSet_truncated[i,:][1]\n",
    "    Target_Users_ALL.append(target_user)\n",
    "    Target_Movie_ALL.append(target_movie)\n",
    "\n",
    "  # Find all the other ratings made by target_user and the corresponding movies\n",
    "    OtherMoviesRated_target_user = (X[X[:,0]==target_user])[:,1] # movies rated\n",
    "    OtherRatings_target_user = (y[X[:,0]==target_user]) # ratings for movies rated\n",
    "    Average_rating_target_user = sum(OtherRatings_target_user )/len(OtherRatings_target_user) #average rating for the target user\n",
    "\n",
    "  ### Find similarity of the target_user with other users\n",
    "  # [1] First, find other users who have also watched (one or  more) of the same movies as the target_user\n",
    "    OtherUsers = [target_user] #Initialize list\n",
    "    for common_movie in OtherMoviesRated_target_user:\n",
    "        OtherUsers_temp = (X[X[:,1]==common_movie])[:,0]\n",
    "        OtherUsers_temp = OtherUsers_temp.tolist()\n",
    "        OtherUsers = OtherUsers + OtherUsers_temp\n",
    "        OtherUsers = list(set(OtherUsers)) #Remove duplicate Users form the list\n",
    "    OtherUsers = np.array(OtherUsers)\n",
    "    OtherUsersTotal = OtherUsers.shape[0] #Total users who shares movies rated with target_user\n",
    "\n",
    "  # [2] Next, create a list of movies that have been rated by both (target_user and the other user)\n",
    "  # Then, use this set to find the similarity between the target user and this user using the Pearson Coefficient\n",
    "    TotalCommonMovies = [] #a list that stores the movies that the target_user has in common with every other user\n",
    "    PearsonCoefficient = []\n",
    "    Average_ratings = [] #a list containing  avg rating for all users\n",
    "    Rating_for_target_movie = [] #a list containing rating by other users for the target_movie\n",
    "    for user in OtherUsers:\n",
    "        movies_rated_ThisUser = (X[X[:,0]==user])[:,1] \n",
    "        movies_rated_ThisUser = movies_rated_ThisUser.tolist()\n",
    "        All_rating_ThisUser = (y[X[:,0]==user]) # ratings for movies rated\n",
    "        avg_rating_ThisUser = sum(All_rating_ThisUser)/len(All_rating_ThisUser)\n",
    "        Average_ratings.append(avg_rating_ThisUser)\n",
    "        All_movies_Set = movies_rated_ThisUser + OtherMoviesRated_target_user.tolist()\n",
    "        common_movies_Set = ([item for item, count in Counter(All_movies_Set).items() if count > 1])\n",
    "        TotalCommonMovies.append(len(common_movies_Set))\n",
    "        target_rating = []\n",
    "        ThisUser_rating = []\n",
    "        Xmovies_for_this_user = (X[X[:,0]==user]) # create a subset of X which contains only movies rated by ThisUser\n",
    "        Yratings_for_this_user = (y[X[:,0]==user]) # Create a subset of y which corresponds to the above subset of X\n",
    "        Rating_target_movie_ThisUser = (Yratings_for_this_user[Xmovies_for_this_user [:,1]==target_movie]) # rating of the target movie by This User\n",
    "        if (len(Rating_target_movie_ThisUser)==0):\n",
    "            Rating_target_movie_ThisUser = 0\n",
    "        else:\n",
    "            Rating_target_movie_ThisUser = Rating_target_movie_ThisUser[0]\n",
    "        Rating_for_target_movie.append(Rating_target_movie_ThisUser)\n",
    "\n",
    "        for mv in common_movies_Set:\n",
    "            ratingUser = (Yratings_for_this_user[Xmovies_for_this_user[:,1]==mv])[0]\n",
    "            ratingTargetUser = OtherRatings_target_user[OtherMoviesRated_target_user==mv][0]\n",
    "            target_rating.append(ratingTargetUser)\n",
    "            ThisUser_rating.append(ratingUser)\n",
    "            target_rating = np.asarray(target_rating)\n",
    "            ThisUser_rating = np.asarray(ThisUser_rating)\n",
    "        if (np.unique(target_rating).shape[0]==1 or np.unique(ThisUser_rating).shape[0]==1):\n",
    "            PearsonCoeff = np.array([[0,0],[0,0]])\n",
    "        else:\n",
    "            PearsonCoeff = np.corrcoef(target_rating, ThisUser_rating)\n",
    "        PearsonCoefficient.append(PearsonCoeff[0,1])\n",
    "\n",
    "  # [3] Post process the Similarity score.\n",
    "  # To this end, we know the similarity score between the target_user and every other user who saw the same movies as the target user\n",
    "  # But the problem is that target_user + UserA may have watched 200 movies together and based on that , the similarity score is 0.2\n",
    "  # And then target_user + UserB may have watched only 10 movies together and based on that, the similarity score is 0.9\n",
    "  # Here we cannot simply conclude that UserB is more similar to target user than UserA.\n",
    "  # So, we have to compensate for the imbalanced data used for finding the similarity scores.\n",
    "    norm_TotalCommonMovies = [float(z)/sum(TotalCommonMovies) for z in TotalCommonMovies]\n",
    "    Corrected_PearsonCoefficient = np.multiply(PearsonCoefficient,norm_TotalCommonMovies)\n",
    "\n",
    "  # [4] Select Top 50 nearest neighbors, whenever avaialble, otherwise just select all the neighbors if neighbors are less than 50\n",
    "    NN = 50\n",
    "    if (np.array(Corrected_PearsonCoefficient).shape[0]<50):\n",
    "        NN = (np.array(Corrected_PearsonCoefficient).shape[0])\n",
    "\n",
    "    TopNNneighers_target_User = np.argpartition(Corrected_PearsonCoefficient, -NN)[-NN:] # This gives the indices of top NN Pearson Coefficients\n",
    "\n",
    "  # [5] Predict the ratings of the target_user using the weighted average of NN nearest neighbors \n",
    "    num=0\n",
    "    den=0\n",
    "    itr=0\n",
    "    for idx in TopNNneighers_target_User:\n",
    "        num = num + Corrected_PearsonCoefficient[idx]* (Rating_for_target_movie[idx] - Average_ratings[idx])\n",
    "        den = den + Corrected_PearsonCoefficient[idx] \n",
    "    prediction = Average_rating_target_user + num/den # This is the predicted rating for the target_user\n",
    "    Total_predictions_UserUserMtd1.append(prediction)\n",
    "\n",
    "  # [6] Find the error in the prediction\n",
    "  # Compare the predicted rating to the actual rating\n",
    "    Actual_rating = (OtherRatings_target_user[OtherMoviesRated_target_user==target_movie])[0]\n",
    "    Actual_rating_ALL.append(Actual_rating) #Store all actual predictions\n",
    "    err = (Actual_rating - prediction)\n",
    "    MAS_cumulative_UserUserMtd1 = MAS_cumulative_UserUserMtd1 + abs(err) \n",
    "    RMSE_cumulative_UserUserMtd1 = RMSE_cumulative_UserUserMtd1+ err*err "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final error for all predictions\n",
    "MAS = MAS_cumulative_UserUserMtd1/len(Total_predictions_UserUserMtd1)\n",
    "RMSE = np.sqrt(RMSE_cumulative_UserUserMtd1/len(Total_predictions_UserUserMtd1))\n",
    "print(\"The Mean Absolute Error for User-User Similarity Collaborative Filtering is: \", MAS)\n",
    "print(\"The Root Mean Squared Error for User-User Similarity Collaborative Filtering is: \", RMSE)\n",
    "\n",
    "print(Total_predictions_UserUserMtd1) #Store all predictions made using Method 1\n",
    "print(Actual_rating_ALL) # Store all actual predictions\n",
    "#print(Target_Users_ALL)\n",
    "#print(Target_Movie_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean Absolute Error for User-User Similarity Collaborative Filtering is:  1.8464365047616664\n",
    "\n",
    "The Root Mean Squared Error for User-User Similarity Collaborative Filtering is:  2.2133092950241102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Movie - Movie Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only Top 5000 users further processing\n",
    "lite_rating_df = pd.DataFrame()\n",
    "group = df.groupby('user_id')['rating'].count()\n",
    "top_users = group.sort_values(ascending=False)[:5000]\n",
    "group = df.groupby('movie_id')['rating'].count()\n",
    "top_movies = group.sort_values(ascending=False)[:100]\n",
    "lite_rating_df = df.join(top_users, rsuffix='_r', how='inner', on='user_id')\n",
    "#lite_rating_df = lite_rating_df.join(top_movies, rsuffix='_r', how='inner', on='movie_id')\n",
    "print(lite_rating_df.nunique())\n",
    "#lite_rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dataframe to numpy matrices\n",
    "X = lite_rating_df[['user_id', 'movie_id']].values\n",
    "y = lite_rating_df['rating'].values\n",
    "print(X.shape)\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pre-processing was necessary because of the pre-processing step that was applied to the Training set\n",
    "# When pre-processing was applied to the training set, only Top-K users were kept\n",
    "# So, some of the data was lost from the training set for the users in the Test Set.\n",
    "# This meant that same pre-processing had to be applied to the Test Set to apply User-User collaborative filtering\n",
    "\n",
    "lite_rating_df_Test = pd.DataFrame()\n",
    "lite_rating_df_Test = df_Test.join(top_users, how='inner', on='user_id')\n",
    "#lite_rating_df_Test = lite_rating_df_Test.join(top_movies, how='inner',lsuffix='_left', rsuffix='_right', on='movie_id')\n",
    "print(lite_rating_df_Test.nunique())\n",
    "#lite_rating_df_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSet = lite_rating_df_Test[['movie_id', 'user_id']].values\n",
    "TestSet_truncated = TestSet[TestSet[:,0]<=1000] \n",
    "#print(TestSet_truncated)\n",
    "print(TestSet_truncated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Unique_users = np.unique(X[:,0])\n",
    "MAS_cumulative_UserUserMtd2 = 0 # Cumulative Error (Mean Absolute Error)\n",
    "RMSE_cumulative_UserUserMtd2 = 0 # Cumulatve Error (Root Mean Squared Error)\n",
    "Total_predictions_UserUserMtd2 = [] #Store all predictions made using Method 2\n",
    "Actual_rating_ALL = [] # Store all actual predictions\n",
    "Target_Users_ALL = []\n",
    "Target_Movie_ALL = []\n",
    "user_movie_matrix = np.zeros((3,5000)) #we have 5000 unique users, so 5000 maximum possible ratings for a particular movie\n",
    "user_movie_matrix[0,:] = Unique_users\n",
    "\n",
    "# Make predictions for all the data in the TestSet_Truncated\n",
    "#for i in range(0, TestSet_truncated.shape[0]):\n",
    "for i in range(0,100):\n",
    "    target_movie = TestSet_truncated[i,:][0]\n",
    "    target_user = TestSet_truncated[i,:][1]\n",
    "    Target_Users_ALL.append(target_user)\n",
    "    Target_Movie_ALL.append(target_movie)\n",
    "\n",
    "  #Find all the other ratings made by target_user and the corresponding movies\n",
    "    OtherMoviesRated_target_user = (X[X[:,0]==target_user])[:,1] # movies rated\n",
    "    OtherRatings_target_user = (y[X[:,0]==target_user]) # ratings for movies rated\n",
    "    Average_rating_target_user = sum(OtherRatings_target_user )/len(OtherRatings_target_user) #average rating for the target user\n",
    "\n",
    "  #### Find all the users who have rated the target_movie and their ratings for this movie\n",
    "    All_ratings_target_movie = (y[X[:,1]==target_movie])# ratings\n",
    "    All_users_target_movie = (X[X[:,1]==target_movie])[:,0]# users\n",
    "    for pp in range(0,len(All_ratings_target_movie)):\n",
    "        row = 1\n",
    "        col = np.where(user_movie_matrix[0,:]==All_users_target_movie[pp])[0][0]\n",
    "        user_movie_matrix[row,col] = All_ratings_target_movie[pp] \n",
    "\n",
    "  #Next, compare target movie with other movies that this target_user has watched.\n",
    "  # [1] Find the Pearson coefficients\n",
    "    Pearson = []\n",
    "    Average_rating_movies = []\n",
    "    for zz in range(0,len(OtherMoviesRated_target_user)):\n",
    "        other_movie = OtherMoviesRated_target_user[zz]\n",
    "        User_who_rated_other_movie = (X[X[:,1]==other_movie])[:,0]#users\n",
    "        Users_ratings_other_movie =  (y[X[:,1]==other_movie]) #ratings\n",
    "        Average_other_movie = sum(Users_ratings_other_movie)/len(Users_ratings_other_movie) #average rating for this movie\n",
    "        Average_rating_movies.append(Average_other_movie)\n",
    "        for nn in range(0,len(User_who_rated_other_movie)):\n",
    "            row = 2\n",
    "            col = np.where(user_movie_matrix[0,:]==User_who_rated_other_movie[nn])[0][0]\n",
    "            user_movie_matrix[row,col] = Users_ratings_other_movie[nn]\n",
    "        #Find Pearson coeffiecnt between the target_movie and other_movie\n",
    "        PearsonCoeff = np.corrcoef(user_movie_matrix[1,:], user_movie_matrix[2,:])\n",
    "        Pearson.append(PearsonCoeff[0,1])\n",
    "\n",
    "  # [2] Select Top 50 nearest neighbors, whenever avaialble, otherwise just select all the neighbors if neighbors are less than 50\n",
    "    NN = 50\n",
    "    if (np.array(Pearson).shape[0]<50):\n",
    "        NN = (np.array(Pearson).shape[0])\n",
    "\n",
    "    TopNNneighers_target_Movie = np.argpartition(Pearson, -NN)[-NN:] # This gives the indices of top NN Pearson Coefficients\n",
    "\n",
    "  # [3] Predict the ratings of the target_user using the weighted average of NN nearest neighbors (movies)\n",
    "    num=0\n",
    "    den=0\n",
    "    itr=0\n",
    "    for idx in TopNNneighers_target_Movie:\n",
    "        num = num + Pearson[idx]* (OtherRatings_target_user[idx] - Average_rating_movies[idx])  \n",
    "        den = den + Pearson[idx] \n",
    "    prediction = Average_rating_target_user + num/den # This is the predicted rating for the target_user\n",
    "    Total_predictions_UserUserMtd2.append(prediction)\n",
    "\n",
    "  # [4] Find the error in the prediction\n",
    "  # Compare the predicted rating to the actual rating\n",
    "    Actual_rating = (OtherRatings_target_user[OtherMoviesRated_target_user==target_movie])[0]\n",
    "    Actual_rating_ALL.append(Actual_rating) #Store all actual predictions\n",
    "    err = (Actual_rating - prediction)\n",
    "    MAS_cumulative_UserUserMtd2 = MAS_cumulative_UserUserMtd2 + abs(err) \n",
    "    RMSE_cumulative_UserUserMtd2 = RMSE_cumulative_UserUserMtd2+ err*err "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final error for all predictions\n",
    "MAS = MAS_cumulative_UserUserMtd2/len(Total_predictions_UserUserMtd2)\n",
    "RMSE = np.sqrt(RMSE_cumulative_UserUserMtd2/len(Total_predictions_UserUserMtd2))\n",
    "print(\"The Mean Absolute Error for Movie-Movie Similarity Collaborative Filtering is: \", MAS)\n",
    "print(\"The Root Mean Squared Error for Movie-Movie Similarity Collaborative Filtering is: \", RMSE)\n",
    "\n",
    "print(Total_predictions_UserUserMtd2) #Store all predictions made using Method 1\n",
    "print(Actual_rating_ALL) # Store all actual predictions\n",
    "print(Target_Users_ALL)\n",
    "print(Target_Movie_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean Absolute Error for Movie-Movie Similarity Collaborative Filtering is:  0.777954443277451\n",
    "    \n",
    "The Root Mean Squared Error for Movie-Movie Similarity Collaborative Filtering is:  1.0604772353326954"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
